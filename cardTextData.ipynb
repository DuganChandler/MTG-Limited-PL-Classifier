{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "json_files = ['AFR.json', 'AKR.json', 'BRO.json', 'DBL.json', 'DMU.json', 'DOM.json', 'ELD.json', 'IKO.json', 'KHM.json', 'WAR.json', \n",
    "              'KLR.json', 'KTK.json', 'LCI.json', 'LTR.json', 'M19.json', 'MAT.json', 'MID.json', 'MKM.json', 'MOM.json', 'NEO.json', \n",
    "              'ONE.json', 'RIX.json', 'RNA.json', 'SIR.json', 'SNC.json', 'STX.json', 'THB.json', 'VOW.json', 'WOE.json', 'STA.json', \n",
    "              'SIS.json', 'M20.json', 'GRN.json', 'M21.json', 'MUL.json', 'WOT.json', 'SPG.json', 'BRR.json', 'MOC.json']\n",
    "def clean_card_text(text, name):\n",
    "    cleaned_text = re.sub(r'\\([^)]*\\)', '', text)\n",
    "    cleaned_text = cleaned_text.replace('\\n', ' || ')\n",
    "    if name:\n",
    "        if '//' in name:\n",
    "            names = name.split(' // ')\n",
    "            for split_name in names:\n",
    "                if split_name in cleaned_text:\n",
    "                    cleaned_text = cleaned_text.replace(split_name, '')\n",
    "        else:\n",
    "            cleaned_text = cleaned_text.replace(name, '')\n",
    "    \n",
    "    cleaned_text = cleaned_text.strip()\n",
    "    cleaned_text = re.sub(' +', ' ', cleaned_text)\n",
    "    cleaned_text = cleaned_text.strip('|')\n",
    "    return cleaned_text \n",
    "\n",
    "dfs = []\n",
    "columns_of_interest = {\n",
    "    \"name\": None,\n",
    "    \"text\": None,\n",
    "    \"types\": None,\n",
    "    \"sub_types\": None, \n",
    "    \"super_types\": None, \n",
    "    \"mana_cost\": None,\n",
    "    \"cmc\": None,\n",
    "    \"power\": None,\n",
    "    \"toughness\": None,\n",
    "}\n",
    "for file in json_files:\n",
    "    with open(f'CardData/{file}', encoding=\"utf8\") as f:\n",
    "        data = pd.read_json(f)\n",
    "\n",
    "        card_data = data['data']['cards']\n",
    "\n",
    "        filtered_cards = []\n",
    "        for card in card_data:\n",
    "            if card.get('isRebalanced') in card.get(\"types\"):\n",
    "                continue\n",
    "            filtered_card = {}\n",
    "            filtered_card['name'] = card.get('name', None)\n",
    "            filtered_card['text'] = card.get('text', \"vanilla\")\n",
    "            filtered_card['types'] = ' '.join(card.get('types', 'none'))\n",
    "            filtered_card['subtypes'] = ' '.join(card.get('subtypes', 'none'))\n",
    "            filtered_card['supertypes'] = card.get('supertypes', 'none')\n",
    "            filtered_card['manaCost'] = card.get('manaCost', 'none').replace('{', \"\").replace('}', '')\n",
    "            filtered_card['cmc'] = card.get('convertedManaCost', None)\n",
    "            filtered_card['power'] = card.get('power', None)\n",
    "            filtered_card['toughness'] = card.get('toughness', None)\n",
    "            filtered_card['layout'] = card.get('layout', None)\n",
    "            if filtered_card['text'] != 'vanilla':\n",
    "                filtered_card['text'] = clean_card_text(filtered_card['text'], filtered_card[\"name\"])\n",
    "            filtered_cards.append(filtered_card)\n",
    "            \n",
    "        df = pd.DataFrame(filtered_cards)\n",
    "        dfs.append(df)\n",
    "\n",
    "final_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "split_aftermath_df = final_df[final_df['layout'].isin(['split', 'aftermath'])]\n",
    "# Combine duplicate entries for 'split' or 'aftermath'\n",
    "combined_df = split_aftermath_df.groupby('name').agg({\n",
    "    'layout': 'first',\n",
    "    'text': ' || '.join,\n",
    "    'types': 'first',  # Join unique types only\n",
    "    'manaCost': lambda x: ' '.join(filter(None, x)),  # Join non-empty mana costs\n",
    "    'cmc': 'first',\n",
    "    'subtypes': 'first',\n",
    "    'power': 'first',\n",
    "    'toughness': 'first',\n",
    "}).reset_index()\n",
    "\n",
    "# Filter original DataFrame to exclude 'split' or 'aftermath' as they are already processed\n",
    "non_split_aftermath_df = final_df[~final_df['layout'].isin(['split', 'aftermath'])]\n",
    "\n",
    "# # Concatenate the processed and non-processed DataFrames\n",
    "final_df = pd.concat([combined_df, non_split_aftermath_df], ignore_index=True)\n",
    "final_df = final_df.applymap(lambda x: x.lower() if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['count'] = final_df.groupby('name').cumcount()\n",
    "result_df = final_df[final_df['count'] < 2]\n",
    "result_df = result_df.drop(columns=['count'])\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_df = result_df.groupby('name').agg({\n",
    "    'layout': 'first',\n",
    "    'text': lambda x: ' '.join(set(x)),\n",
    "    'types': 'first',  # Join unique types only\n",
    "    'manaCost': lambda x: ' '.join(set(filter(None, x))),  # Join non-empty mana costs\n",
    "    'cmc': 'first',\n",
    "    'subtypes': 'first',\n",
    "    'power': 'first',\n",
    "    'toughness': 'first',\n",
    "}).reset_index()\n",
    "\n",
    "def conditional_split(row):\n",
    "    if row['layout'] in ['aftermath', \"split\"]:\n",
    "        return row['name']\n",
    "    else:\n",
    "        parts = row['name'].split(' // ')\n",
    "        return parts[0]\n",
    "comb_df['name'] = comb_df.apply(conditional_split, axis=1) \n",
    "\n",
    "data_df = pd.read_csv(\"CardData/CleanedCombinedSetsPowerLevel.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['Name'] = data_df['Name'].str.lower()\n",
    "data_df = data_df[~data_df['Name'].str.startswith('a-')]\n",
    "names_comb = set(comb_df['name'])\n",
    "names_data = set(data_df['Name'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_df = comb_df[comb_df['name'].isin(data_df['Name'])]\n",
    "names_comb = set(comb_df['name'])\n",
    "names_data = set(data_df['Name'])\n",
    "\n",
    "missing_names = names_data - names_comb\n",
    "#missing_names = names_comb - names_data\n",
    "print(missing_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_columns = ['Name', 'power_level']\n",
    "filtered_df = data_df[desired_columns]\n",
    "filtered_df = filtered_df.rename(columns={'Name': 'name'})\n",
    "final_combined_df = pd.merge(filtered_df, comb_df, on='name', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_combined_df.head())\n",
    "print(final_combined_df.info())\n",
    "print(final_combined_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_combined_df['power'].fillna(0, inplace=True)\n",
    "final_combined_df['toughness'].fillna(0, inplace=True)\n",
    "final_combined_df['subtypes'].replace('', 'none', inplace=True)\n",
    "final_combined_df['manaCost'].fillna('none', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import contractions\n",
    "import re\n",
    "\n",
    "df = final_combined_df\n",
    "\n",
    "def remove_braces(text):\n",
    "    return re.sub(r'\\{.*?\\}', '', text)\n",
    "\n",
    "def remove_contractions(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "def parse_mana_cost(df):\n",
    "    generic_mana = re.compile(r'\\d+')\n",
    "\n",
    "    mana_types = ['generic_mana', 'x', 'w', 'u', 'b', 'r', 'g']\n",
    "    for mana in mana_types:\n",
    "        df[mana] = 0\n",
    "\n",
    "    def parse_each(mana_cost):\n",
    "        if pd.isna(mana_cost):\n",
    "            return\n",
    "        generic = generic_mana.search(mana_cost)\n",
    "        if generic:\n",
    "            df.at[index, 'generic_mana'] = int(generic.group())\n",
    "        # Subtract the generic mana number length to avoid counting numbers as colors\n",
    "        color_part = generic_mana.sub('', mana_cost)\n",
    "        for char in color_part:\n",
    "            if char in mana_types:\n",
    "                df.at[index, char] += 1\n",
    "\n",
    "    # Apply parsing function to each row\n",
    "    for index, row in df.iterrows():\n",
    "        parse_each(row['manaCost'])\n",
    "\n",
    "    return df\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "df.fillna({'subtypes': 'none', 'power': 0, 'toughness': 0}, inplace=True)\n",
    "\n",
    "df = parse_mana_cost(df)\n",
    "for col in ['cmc', 'power', 'toughness', 'generic_mana', 'w', 'u', 'b', 'r', 'g']:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "df.fillna(0, inplace=True) \n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "numeric_columns = ['cmc', 'power', 'toughness', 'generic_mana', 'w', 'u', 'b', 'r', 'g']\n",
    "df[numeric_columns] = scaler.fit_transform(df[numeric_columns])\n",
    "\n",
    "enc = OneHotEncoder(sparse_output=False)\n",
    "#categorical_columns = ['layout', 'types', 'subtypes']\n",
    "categorical_columns = ['types', 'layout']\n",
    "categorical_data = enc.fit_transform(df[categorical_columns])\n",
    "categorical_df = pd.DataFrame(categorical_data, columns=enc.get_feature_names_out(categorical_columns))\n",
    "\n",
    "df['text'] = df['text'].apply(remove_braces)\n",
    "df['text'] = df['text'].apply(remove_contractions)\n",
    "df['text'] = df['text'].str.replace(r'//', ' ', regex=True)\n",
    "df['text'] = df['text'].str.replace('[^\\w\\s]', '', regex=True)\n",
    "df['text'] = df['text'].str.replace('\\d+', '', regex=True)\n",
    "\n",
    "# Text vectorization\n",
    "vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
    "text_features = vectorizer.fit_transform(df['text'])\n",
    "text_df = pd.DataFrame(text_features.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Combine all preprocessed data\n",
    "processed_df = pd.concat([df[numeric_columns],df[categorical_columns], text_df], axis=1)\n",
    "processed_df['power_level'] = df['power_level']  # Add target variable\n",
    "# Split dataset into train and test sets\n",
    "X = processed_df.drop('power_level', axis=1)\n",
    "y = df['power_level']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.17, random_state=42)\n",
    "\n",
    "# Output the first few rows of the training set to verify\n",
    "print(X_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MTGClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MTGClassifier, self).__init__()\n",
    "        # Initialize layers based on the actual number of features\n",
    "        self.fc_numeric = nn.Linear(11, 32)  # Numeric features\n",
    "        self.fc_categorical = nn.Linear(23, 32)  # Categorical features\n",
    "        self.fc_text = nn.Linear(1242, 128)  # Text features\n",
    "\n",
    "        # Concatenate all features: 32 from numeric, 32 from categorical, 64 from text\n",
    "        self.fc1 = nn.Linear(32 + 32 + 128, 192)\n",
    "        self.fc2 = nn.Linear(192, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 5)  # Output layer for 5 classes\n",
    "\n",
    "    def forward(self, x_numeric, x_categorical, x_text):\n",
    "        x_numeric = F.relu(self.fc_numeric(x_numeric))\n",
    "        x_categorical = F.relu(self.fc_categorical(x_categorical))\n",
    "        x_text = F.relu(self.fc_text(x_text))\n",
    "\n",
    "        # Concatenate all features\n",
    "        x = torch.cat((x_numeric, x_categorical, x_text), dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, p=0.3, training=self.training)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.dropout(x, p=0.3, training=self.training)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataframes / arrays to tensors\n",
    "X_numeric_train = torch.tensor(X_train[numeric_columns].values, dtype=torch.float32)\n",
    "X_categorical_train = torch.tensor(categorical_df.loc[X_train.index].values, dtype=torch.float32)\n",
    "X_text_train = torch.tensor(text_df.loc[X_train.index].values, dtype=torch.float32)\n",
    "\n",
    "X_numeric_test= torch.tensor(X_test[numeric_columns].values, dtype=torch.float32)\n",
    "X_categorical_test= torch.tensor(categorical_df.loc[X_test.index].values, dtype=torch.float32)\n",
    "X_text_test= torch.tensor(text_df.loc[X_test.index].values, dtype=torch.float32)\n",
    "\n",
    "y_train = torch.tensor(y_train.values, dtype=torch.long)\n",
    "y_test = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "# Similar conversion for X_test if needed for evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Create dataset and data loader\n",
    "train_dataset = TensorDataset(X_numeric_train, X_categorical_train, X_text_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Initialize the model\n",
    "model = MTGClassifier()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epochs = 14\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    for numeric, categorical, text, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(numeric, categorical, text)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorDataset for testing\n",
    "test_dataset = TensorDataset(X_numeric_test, X_categorical_test, X_text_test, y_test)\n",
    "\n",
    "# Create DataLoader for testing\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)  # No need to shuffle for testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():  # Turn off gradients for validation, saves memory and computations\n",
    "    for numeric, categorical, text, labels in test_loader:\n",
    "        outputs = model(numeric, categorical, text)\n",
    "        _, predicted = torch.max(outputs.data, 1)  # Get the indices of the max log-probability\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Accuracy of the model on the test data: {accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchviz import make_dot\n",
    "m = model(X_numeric_test, X_categorical_test, X_text_test)\n",
    "make_dot(m.mean(), params=dict(model.named_parameters()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
